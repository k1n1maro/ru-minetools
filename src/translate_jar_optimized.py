#!/usr/bin/env python3
"""
–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ JAR –º–æ–¥–æ–≤ Minecraft
–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 5-10 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–µ–π
"""

import os
import json
import zipfile
import tempfile
import shutil
import time
import pickle
import hashlib
import sqlite3
import re
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from translatepy import Translator
from functools import lru_cache
from typing import List, Dict, Tuple, Optional, Set
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ö–û–ù–°–¢–ê–ù–¢–´
BATCH_SIZE = 50  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 10 –¥–æ 50
MAX_BATCH_LENGTH = 4000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–∞–∫–µ—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
DELAY_BETWEEN_BATCHES = 0.3  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 1.0 –¥–æ 0.3 —Å–µ–∫—É–Ω–¥—ã
MAX_WORKERS = 3  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ JAR —Ñ–∞–π–ª–æ–≤

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
translator = Translator()

# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
TECHNICAL_PATTERN = re.compile(r'^[a-z_]+:[a-z_]+$')  # minecraft:stone
PLACEHOLDER_PATTERN = re.compile(r'[{}%]')  # {player}, %d
CYRILLIC_PATTERN = re.compile(r'[\u0400-\u04FF]')  # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞
FORMATTING_PATTERN = re.compile(r'[¬ß&][0-9a-fk-or]')  # ¬ßa, &c

class OptimizedTranslationCache:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤ —Å SQLite"""
    
    def __init__(self, db_path="translation_cache_optimized.db"):
        self.db_path = db_path
        self.memory_cache = {}  # LRU –∫—ç—à –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self.init_db()
    
    def init_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS translations (
                    source_hash TEXT PRIMARY KEY,
                    source_text TEXT,
                    target_lang TEXT,
                    translated_text TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_hash ON translations(source_hash)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_lang ON translations(target_lang)")
    
    def get_cache_key(self, text: str, lang_to: str = 'ru') -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∞"""
        return hashlib.md5(f"{text}:{lang_to}".encode()).hexdigest()
    
    def get_batch(self, texts: List[str], lang_to: str = 'ru') -> Tuple[List[Optional[str]], List[int]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã –ø–∞–∫–µ—Ç–æ–º –∏–∑ –∫—ç—à–∞"""
        if not texts:
            return [], []
        
        hashes = [self.get_cache_key(text, lang_to) for text in texts]
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
        results = []
        uncached_indices = []
        db_queries = []
        
        for i, (text, hash_key) in enumerate(zip(texts, hashes)):
            if hash_key in self.memory_cache:
                results.append(self.memory_cache[hash_key])
            else:
                results.append(None)
                uncached_indices.append(i)
                db_queries.append(hash_key)
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–º
        if db_queries:
            with sqlite3.connect(self.db_path) as conn:
                placeholders = ','.join('?' * len(db_queries))
                cursor = conn.execute(
                    f"SELECT source_hash, translated_text FROM translations WHERE source_hash IN ({placeholders})",
                    db_queries
                )
                
                cached_from_db = dict(cursor.fetchall())
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø–∞–º—è—Ç—å
                new_uncached = []
                for i, hash_key in zip(uncached_indices, db_queries):
                    if hash_key in cached_from_db:
                        translation = cached_from_db[hash_key]
                        results[i] = translation
                        self.memory_cache[hash_key] = translation
                    else:
                        new_uncached.append(i)
                
                uncached_indices = new_uncached
        
        return results, uncached_indices
    
    def save_batch(self, texts: List[str], translations: List[str], lang_to: str = 'ru'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã –ø–∞–∫–µ—Ç–æ–º"""
        if not texts or not translations or len(texts) != len(translations):
            return
        
        data = []
        for text, translation in zip(texts, translations):
            hash_key = self.get_cache_key(text, lang_to)
            data.append((hash_key, text, lang_to, translation))
            self.memory_cache[hash_key] = translation
        
        with sqlite3.connect(self.db_path) as conn:
            conn.executemany(
                "INSERT OR REPLACE INTO translations (source_hash, source_text, target_lang, translated_text) VALUES (?, ?, ?, ?)",
                data
            )
            conn.commit()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫—ç—à–∞
cache = OptimizedTranslationCache()

def should_translate(text: str) -> bool:
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–∫"""
    if not text or len(text) < 3 or not text.strip():
        return False
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–≥–µ–∫—Å—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    if (TECHNICAL_PATTERN.match(text) or 
        PLACEHOLDER_PATTERN.search(text) or
        CYRILLIC_PATTERN.search(text) or
        FORMATTING_PATTERN.search(text)):
        return False
    
    return True

def create_smart_batches(texts: List[str], max_length: int = MAX_BATCH_LENGTH) -> List[List[str]]:
    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞–∫–µ—Ç—ã –ø–æ –¥–ª–∏–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤"""
    if not texts:
        return []
    
    batches = []
    current_batch = []
    current_length = 0
    
    for text in texts:
        text_length = len(text) + 15  # +15 –¥–ª—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è " |SEPARATOR| "
        
        if current_length + text_length > max_length and current_batch:
            batches.append(current_batch)
            current_batch = [text]
            current_length = text_length
        else:
            current_batch.append(text)
            current_length += text_length
    
    if current_batch:
        batches.append(current_batch)
    
    return batches

def batch_translate_optimized(texts: List[str], lang_to: str = 'ru', delay: float = DELAY_BETWEEN_BATCHES) -> Tuple[List[str], Dict]:
    """
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ø–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å —É–º–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    """
    if not texts:
        return [], {'cache_hits': 0, 'new_translations': 0, 'total_strings': 0}
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    filtered_texts = []
    original_indices = []
    
    for i, text in enumerate(texts):
        if should_translate(text):
            filtered_texts.append(text)
            original_indices.append(i)
    
    logger.info(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {len(filtered_texts)}/{len(texts)} —Å—Ç—Ä–æ–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
    results = texts.copy()
    
    if not filtered_texts:
        return results, {'cache_hits': 0, 'new_translations': 0, 'total_strings': len(texts)}
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –∫—ç—à–∞ –ø–∞–∫–µ—Ç–æ–º
    cached_results, uncached_indices = cache.get_batch(filtered_texts, lang_to)
    cache_hits = sum(1 for r in cached_results if r is not None)
    
    logger.info(f"–ö—ç—à: {cache_hits}/{len(filtered_texts)} –ø–æ–ø–∞–¥–∞–Ω–∏–π")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for i, cached_result in enumerate(cached_results):
        if cached_result is not None:
            original_index = original_indices[i]
            results[original_index] = cached_result
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –Ω–µ–ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    uncached_texts = [filtered_texts[i] for i in uncached_indices]
    
    if uncached_texts:
        logger.info(f"–ü–µ—Ä–µ–≤–æ–¥–∏–º {len(uncached_texts)} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫...")
        
        # –°–æ–∑–¥–∞–µ–º —É–º–Ω—ã–µ –ø–∞–∫–µ—Ç—ã
        smart_batches = create_smart_batches(uncached_texts)
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(smart_batches)} –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤")
        
        translated_texts = []
        
        for batch_num, batch in enumerate(smart_batches, 1):
            try:
                if delay > 0 and batch_num > 1:
                    time.sleep(delay)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–∞–∫–µ—Ç
                batch_text = " |SEPARATOR| ".join(batch)
                logger.info(f"–ü–∞–∫–µ—Ç {batch_num}/{len(smart_batches)}: {len(batch)} —Å—Ç—Ä–æ–∫, {len(batch_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ü–µ—Ä–µ–≤–æ–¥–∏–º
                translated_batch = str(translator.translate(batch_text, lang_to))
                
                # –†–∞–∑–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                translated_parts = translated_batch.split(" |SEPARATOR| ")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
                if len(translated_parts) != len(batch):
                    logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–∞: {len(translated_parts)} != {len(batch)}")
                    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
                    translated_parts = []
                    for text in batch:
                        try:
                            if delay > 0:
                                time.sleep(delay * 0.3)
                            translated = str(translator.translate(text, lang_to))
                            translated_parts.append(translated)
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ '{text}': {e}")
                            translated_parts.append(text)
                
                # –û—á–∏—â–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã
                cleaned_translations = [t.replace('"', "''") for t in translated_parts]
                translated_texts.extend(cleaned_translations)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                translated_texts.extend(batch)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –ø–∞–∫–µ—Ç–æ–º
        if len(translated_texts) == len(uncached_texts):
            cache.save_batch(uncached_texts, translated_texts, lang_to)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        for i, translated in enumerate(translated_texts):
            if i < len(uncached_indices):
                filtered_index = uncached_indices[i]
                original_index = original_indices[filtered_index]
                results[original_index] = translated
    
    stats = {
        'cache_hits': cache_hits,
        'new_translations': len(uncached_texts),
        'total_strings': len(texts),
        'filtered_strings': len(filtered_texts),
        'api_batches': len(create_smart_batches(uncached_texts)) if uncached_texts else 0
    }
    
    return results, stats

def extract_all_strings_from_jar(jar_path: Path) -> Set[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ JAR —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    strings = set()
    
    try:
        with zipfile.ZipFile(jar_path, 'r') as jar:
            # –ò—â–µ–º lang —Ñ–∞–π–ª—ã
            for file_info in jar.filelist:
                if '/lang/' in file_info.filename and file_info.filename.endswith('.json'):
                    try:
                        with jar.open(file_info.filename) as f:
                            content = json.load(f)
                            if isinstance(content, dict):
                                strings.update(content.values())
                    except:
                        continue
            
            # –ò—â–µ–º patchouli —Ñ–∞–π–ª—ã
            for file_info in jar.filelist:
                if '/patchouli_books/' in file_info.filename and file_info.filename.endswith('.json'):
                    try:
                        with jar.open(file_info.filename) as f:
                            content = json.load(f)
                            extract_strings_from_json(content, strings)
                    except:
                        continue
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ –∏–∑ {jar_path}: {e}")
    
    return strings

def extract_strings_from_json(obj, strings: Set[str]):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ JSON –æ–±—ä–µ–∫—Ç–∞"""
    if isinstance(obj, dict):
        for value in obj.values():
            if isinstance(value, str):
                strings.add(value)
            elif isinstance(value, (dict, list)):
                extract_strings_from_json(value, strings)
    elif isinstance(obj, list):
        for item in obj:
            if isinstance(item, str):
                strings.add(item)
            elif isinstance(item, (dict, list)):
                extract_strings_from_json(item, strings)

def preload_and_translate_all_strings(jar_files: List[Path], lang_to: str = 'ru') -> Dict[str, str]:
    """
    –ú–ï–ì–ê-–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    """
    logger.info("üöÄ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫...")
    
    all_strings = set()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—Å–µ—Ö JAR —Ñ–∞–π–ª–æ–≤
    for jar_file in jar_files:
        strings = extract_all_strings_from_jar(jar_file)
        all_strings.update(strings)
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(strings)} —Å—Ç—Ä–æ–∫ –∏–∑ {jar_file.name}")
    
    unique_strings = list(all_strings)
    logger.info(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {len(unique_strings)}")
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ–¥–Ω–∏–º –±–æ–ª—å—à–∏–º –ø–∞–∫–µ—Ç–æ–º
    translated_strings, stats = batch_translate_optimized(unique_strings, lang_to)
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–≤–æ–¥–æ–≤
    translation_dict = dict(zip(unique_strings, translated_strings))
    
    logger.info(f"‚úÖ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω:")
    logger.info(f"   üì¶ –ö—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–π: {stats['cache_hits']}")
    logger.info(f"   üåê –ù–æ–≤—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤: {stats['new_translations']}")
    logger.info(f"   üìä API –ø–∞–∫–µ—Ç–æ–≤: {stats.get('api_batches', 0)}")
    
    return translation_dict

def translate_jar_optimized(jar_path: Path, output_path: Path, lang_to: str = 'ru', 
                          translation_dict: Optional[Dict[str, str]] = None,
                          progress_callback=None, stop_callback=None) -> Dict:
    """
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ JAR —Ñ–∞–π–ª–∞
    """
    logger.info(f"üîÑ –ü–µ—Ä–µ–≤–æ–¥–∏–º {jar_path.name}...")
    
    jar_path = Path(jar_path)
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    output_jar = output_path / f"{jar_path.stem}_ru.jar"
    
    stats = {
        'lang_files': 0,
        'patchouli_files': 0,
        'strings_translated': 0,
        'cache_hits': 0,
        'new_translations': 0
    }
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_dir = Path(temp_dir)
        
        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º JAR
        with zipfile.ZipFile(jar_path, 'r') as jar:
            jar.extractall(temp_dir)
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º lang —Ñ–∞–π–ª—ã
        lang_files_processed = process_lang_files_optimized(
            temp_dir, lang_to, translation_dict, progress_callback, stop_callback
        )
        stats.update(lang_files_processed)
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º patchouli —Ñ–∞–π–ª—ã
        patchouli_files_processed = process_patchouli_files_optimized(
            temp_dir, lang_to, translation_dict, progress_callback, stop_callback
        )
        stats['patchouli_files'] += patchouli_files_processed.get('patchouli_files', 0)
        stats['strings_translated'] += patchouli_files_processed.get('strings_translated', 0)
        
        # –£–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ JAR
        with zipfile.ZipFile(output_jar, 'w', zipfile.ZIP_DEFLATED) as new_jar:
            for root, dirs, files in os.walk(temp_dir):
                for file in files:
                    file_path = Path(root) / file
                    arc_name = file_path.relative_to(temp_dir)
                    new_jar.write(file_path, arc_name)
    
    logger.info(f"‚úÖ {jar_path.name} –ø–µ—Ä–µ–≤–µ–¥–µ–Ω: {stats['strings_translated']} —Å—Ç—Ä–æ–∫")
    return stats

def process_lang_files_optimized(temp_dir: Path, lang_to: str, 
                                translation_dict: Optional[Dict[str, str]] = None,
                                progress_callback=None, stop_callback=None) -> Dict:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ lang —Ñ–∞–π–ª–æ–≤"""
    stats = {'lang_files': 0, 'strings_translated': 0, 'cache_hits': 0, 'new_translations': 0}
    
    # –ò—â–µ–º –≤—Å–µ lang —Ñ–∞–π–ª—ã
    lang_files = list(temp_dir.rglob('**/lang/*.json'))
    en_us_files = [f for f in lang_files if f.name == 'en_us.json']
    
    for lang_file in en_us_files:
        if stop_callback and stop_callback():
            break
        
        try:
            with open(lang_file, 'r', encoding='utf-8') as f:
                content = json.load(f)
            
            if not isinstance(content, dict):
                continue
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –µ—Å–ª–∏ –µ—Å—Ç—å
            if translation_dict:
                translated_content = {}
                for key, value in content.items():
                    translated_content[key] = translation_dict.get(value, value)
                    if translation_dict.get(value, value) != value:
                        stats['strings_translated'] += 1
                        stats['cache_hits'] += 1
            else:
                # –ü–µ—Ä–µ–≤–æ–¥–∏–º –æ–±—ã—á–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
                texts = list(content.values())
                translated_texts, translate_stats = batch_translate_optimized(texts, lang_to)
                
                translated_content = {}
                for key, translated in zip(content.keys(), translated_texts):
                    translated_content[key] = translated
                
                stats['strings_translated'] += translate_stats['new_translations']
                stats['cache_hits'] += translate_stats['cache_hits']
                stats['new_translations'] += translate_stats['new_translations']
            
            # –°–æ–∑–¥–∞–µ–º ru_ru.json —Ñ–∞–π–ª
            ru_file = lang_file.parent / 'ru_ru.json'
            with open(ru_file, 'w', encoding='utf-8') as f:
                json.dump(translated_content, f, ensure_ascii=False, indent=2)
            
            stats['lang_files'] += 1
            
            if progress_callback:
                progress_callback(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω {lang_file.name}")
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {lang_file}: {e}")
    
    return stats

def process_patchouli_files_optimized(temp_dir: Path, lang_to: str,
                                    translation_dict: Optional[Dict[str, str]] = None,
                                    progress_callback=None, stop_callback=None) -> Dict:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ patchouli —Ñ–∞–π–ª–æ–≤"""
    stats = {'patchouli_files': 0, 'strings_translated': 0}
    
    # –ò—â–µ–º patchouli —Ñ–∞–π–ª—ã
    patchouli_files = list(temp_dir.rglob('**/patchouli_books/**/en_us/*.json'))
    
    for patchouli_file in patchouli_files:
        if stop_callback and stop_callback():
            break
        
        try:
            with open(patchouli_file, 'r', encoding='utf-8') as f:
                content = json.load(f)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏
            strings = set()
            extract_strings_from_json(content, strings)
            
            if translation_dict:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
                def translate_json_recursive(obj):
                    if isinstance(obj, dict):
                        return {k: translate_json_recursive(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [translate_json_recursive(item) for item in obj]
                    elif isinstance(obj, str):
                        return translation_dict.get(obj, obj)
                    else:
                        return obj
                
                translated_content = translate_json_recursive(content)
            else:
                # –û–±—ã—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
                translated_content = content
            
            # –°–æ–∑–¥–∞–µ–º ru_ru —Ñ–∞–π–ª
            ru_file = patchouli_file.parent.parent / 'ru_ru' / patchouli_file.name
            ru_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(ru_file, 'w', encoding='utf-8') as f:
                json.dump(translated_content, f, ensure_ascii=False, indent=2)
            
            stats['patchouli_files'] += 1
            stats['strings_translated'] += len(strings)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {patchouli_file}: {e}")
    
    return stats

def translate_jars_parallel_optimized(jar_files: List[Path], output_path: Path, 
                                    lang_to: str = 'ru', max_workers: int = MAX_WORKERS,
                                    progress_callback=None, stop_callback=None) -> Dict:
    """
    –ì–õ–ê–í–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ JAR —Ñ–∞–π–ª–æ–≤
    """
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ {len(jar_files)} JAR —Ñ–∞–π–ª–æ–≤...")
    
    # –≠—Ç–∞–ø 1: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫
    translation_dict = preload_and_translate_all_strings(jar_files, lang_to)
    
    # –≠—Ç–∞–ø 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ JAR —Ñ–∞–π–ª–æ–≤
    total_stats = {
        'jars_processed': 0,
        'jars_failed': 0,
        'lang_files': 0,
        'patchouli_files': 0,
        'strings_translated': 0,
        'cache_hits': 0,
        'new_translations': 0
    }
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        
        for jar_file in jar_files:
            if stop_callback and stop_callback():
                break
            
            future = executor.submit(
                translate_jar_optimized, 
                jar_file, 
                output_path, 
                lang_to, 
                translation_dict,
                progress_callback, 
                stop_callback
            )
            futures[future] = jar_file
        
        for future in as_completed(futures):
            jar_file = futures[future]
            
            try:
                stats = future.result()
                total_stats['jars_processed'] += 1
                total_stats['lang_files'] += stats.get('lang_files', 0)
                total_stats['patchouli_files'] += stats.get('patchouli_files', 0)
                total_stats['strings_translated'] += stats.get('strings_translated', 0)
                total_stats['cache_hits'] += stats.get('cache_hits', 0)
                total_stats['new_translations'] += stats.get('new_translations', 0)
                
                if progress_callback:
                    progress_callback(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω {jar_file.name}")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {jar_file}: {e}")
                total_stats['jars_failed'] += 1
                
                if progress_callback:
                    progress_callback(f"‚ùå –û—à–∏–±–∫–∞ {jar_file.name}: {e}")
    
    logger.info("üéâ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {total_stats}")
    
    return total_stats

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
def translate_jar(jar_path, output_path, lang_to='ru', replace_original=False, 
                 progress_callback=None, stop_callback=None):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API"""
    return translate_jar_optimized(
        Path(jar_path), 
        Path(output_path), 
        lang_to, 
        None,  # –ë–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
        progress_callback, 
        stop_callback
    )

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    jar_files = [Path("example.jar")]
    output_path = Path("translated_jars")
    
    stats = translate_jars_parallel_optimized(jar_files, output_path)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {stats}")